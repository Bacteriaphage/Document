chapter 1.
  Three important factors for a system: reliability, scalability, maintainability
  Reliablity: 
    fault vs failure: single component/user/etc. can make fault, if that cause whole system down, that's a failure.
    We can deliberately introduce fault to system to make it resilient/fault-tolerant.
    idealy, we better reduce faults in each component, but in reality, we have to deal with them.
    Hardware error: add redundancy or have software to handle faults
    Software error: hard to find, can cause cascading failures, better to carefully think when design, thorough testing, allowing crash and restart
                    monitoring, analyzing system behavior in production. Look for guarantee like sum check, sequence check to make self check.
    Human error: human error is the leading cause of outage comparing to software and hardware. We need to design system in a way that easy for 
                 human to do the right thing instead of the wrong thing, which refer to a well API design, a user-friendly UI etc.
                 Sandbox is important for detacting human error. Thoroughly test at all levels, Quick and easy recovery(rollback)
                 Monitoring, good management practice.
  Scalability:
    Before working on scalability, the first thing is to understand what is current load.
    learning case: Twitter
      In 2012, all twitter users publish 4.6k/s tweets on average, 12k/s tweets at peak. Each user has 75 followers on average, millions followers for celebrities.
      Users make 300k/s read requests.
      When a User open twitter, he/she will read tweets in home timeline, both from normal user and celebrities. To execute a query to join follows/tweets/users will
      space lots of time. A alternative way is to push new tweets to user's own cache, but for celebrities who has millions of followers, this will introduce
      more problems.
      So twitter use hybrid mode which using normal join for celebrities, using cache for normal users.
    Describe performance:
      Think about two things: 1. when load parameters goes up, what happened if resource doesn't change? 2. how much new resource we need to fulfill new load parameters
      A batch processing system more focuses on throughput, online system more focuses on response time. We can decouple those two in some use case likes
      Exchange Trading System. Matching engine itself is a batch process system, and the module to get market data and send information back to clients is 
      a online system.
      Percentile/median is better than mean when describe performance. It tells us how many user request are slower than a number(median).
      The response time are different between what client see and what the system see, it is because a slow culprit will make all later client requests wait. System
      only calculate response time as normal, but client actually feels the slowness. So we have two ways to measure performace a little bit accurate:
        1. monitor performance on client side, 2. Sending request independently, don't wait previous request to be finished.
      To calculate percentiles in a sliding window, add up data in a time window and get the result, or use forward decay, t-digest, HdrHistogram.
    Cope increasing load:
      Use a more powerful machine or employ more small elastic machine? Is the service stateless or stateful? Which part of the system is the bottleneck? How about
      maintainance effort we need for new resource?

chapter 2.
  Data model (relational or document) and query language
  Relational DB vs Document DB:
    If application has lots of one to many data, then Document DB (NoSQL) is a good choice, no join is needed and data are easy to be retrieved.
    If we don't a have a fixed schema, we better to use NoSQL database, which is the schema-on-read pattern. It's like RTTI in C++, we cannot assume schema in databse
    until we are reading it. In contrust, relational databse is schema-on-write, we have to follow certain rules when writing.
    When we need to change schema often, NoSQL is preferred in terms of the slowness when altering table in some relational DB. They probably require some downtime.
    If we access set of data in locality pattern, we can use NoSQL, because to join different tables, access indices will require lots of disk IO. NoSQL can
    provide good locality attribute. However, if the amount of data we query is relatively small comparing to cache or we will modify the size of encoded data, it will
    be expensive to achieve them in NoSQL.
  Nowaday, relational and document DB are more and more complementing each other, relational DB has no schema JSON and NoSQL DB resolve relational reference. This 
  hybrid of two models is good route for databases in the future. 
  Query Language:
    declarative query language (SQL, relational algebra, CSS, MapReduce lang, Cypher node4j) vs imperative language
    MapReduce is used by distributed data storage, but not the essential tool. SQL does not constrain to only run in a single node.
    MongoDB use MapReduce, but also have tools like aggregation pipeline to deal with distributed use cases.
    Graph-like data module use vertices and edge to store data and their relation. Any two vertices can connect with each other and user can traverse forward and 
    backward. The relationship of edges can be different, so a graph can represent different information.
    Semantic web and RDF data model: RDF is a machine readible web which similar to the information on the internet which human can understand. There are lots of
    work to be done like standard proposal, complex, etc. (page 57)
chapter 3.
  Hash index: build hash index upon different field can boost reading speed, database engine will figure out which index is needed. However, we cannot store all
    index in memory, we group part of indices into segments and write them into disk. In the meantime, data in segment would have duplication, we need to compact
    them.
    The disadvantage of hash index are 1. when index is in disk, the performance is not very good. 2. when query range of data, it will need to scan the whole hash 
    table
  SSTable and LSM-Trees:
    1. the k-v pairs need to be sorted by key in a segment and one key only appear once in a segment file after merging
    2. We only keep a sparse index table (sorted) in memory, each index point to the offset of key within a sorted segment file(SSTable) which is a small block. 
       If the queried key is in between of two sparse indices, we only need to load the small compressed data block in SSTable
    Steps of building SSTable:
      1. write new data into an in-memory sorted balanced tree which we call memtable.
      2. When the size of memtable exceed threshold, we write it into a SSTable and store it on the disk.
      3. To find a record, we check memtable first, then the SSTable in starting from the latest.
      4. Merge can compact SSTable in the background to save space.
      5. To avoid losing memtable when crash, store log for memtable write without order into disk.
    Performance improvement:
      Bloom filters when read
      Size-tiered compaction is for write intensive db, several small SSTable will be compacted into a larger one, this need 2x size of compacted data disk space.
      Leveled compaction will compact L0 SSTable to higher level continuously. There are two parameters 1. max SSTable size, 2. max size of LevelX.
      https://www.youtube.com/watch?v=TyTXOjFMi7k&t=617s&ab_channel=DataStaxDevelopers
      Max size of LevelX is max SSTable size times X power of 10. L0 is a temporary level to store data, compactor will merge data from all SSTables in L0 and merge
      new data with L1 SSTable, write them into a file not exceeding Max SSTable size and store them into L1. If the size of L1 exceed max size of Lv1, promote and
      merge SSTables in Lv1 with Lv2 SSTables until remaining table in Lv1 fit in max size of L1 and so on.
      https://www.youtube.com/watch?v=6yJEwqseMY4&ab_channel=DataStaxDevelopers
      This is good for reading, when writing happenes, it is very IO intensive
