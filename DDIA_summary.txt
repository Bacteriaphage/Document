chapter 1.
  Three important factors for a system: reliability, scalability, maintainability
  Reliablity: 
    fault vs failure: single component/user/etc. can make fault, if that cause whole system down, that's a failure.
    We can deliberately introduce fault to system to make it resilient/fault-tolerant.
    idealy, we better reduce faults in each component, but in reality, we have to deal with them.
    Hardware error: add redundancy or have software to handle faults
    Software error: hard to find, can cause cascading failures, better to carefully think when design, thorough testing, allowing crash and restart
                    monitoring, analyzing system behavior in production. Look for guarantee like sum check, sequence check to make self check.
    Human error: human error is the leading cause of outage comparing to software and hardware. We need to design system in a way that easy for 
                 human to do the right thing instead of the wrong thing, which refer to a well API design, a user-friendly UI etc.
                 Sandbox is important for detacting human error. Thoroughly test at all levels, Quick and easy recovery(rollback)
                 Monitoring, good management practice.
  Scalability:
    Before working on scalability, the first thing is to understand what is current load.
    learning case: Twitter
      In 2012, all twitter users publish 4.6k/s tweets on average, 12k/s tweets at peak. Each user has 75 followers on average, millions followers for celebrities.
      Users make 300k/s read requests.
      When a User open twitter, he/she will read tweets in home timeline, both from normal user and celebrities. To execute a query to join follows/tweets/users will
      space lots of time. A alternative way is to push new tweets to user's own cache, but for celebrities who has millions of followers, this will introduce
      more problems.
      So twitter use hybrid mode which using normal join for celebrities, using cache for normal users.
    Describe performance:
      Think about two things: 1. when load parameters goes up, what happened if resource doesn't change? 2. how much new resource we need to fulfill new load parameters
      A batch processing system more focuses on throughput, online system more focuses on response time. We can decouple those two in some use case likes
      Exchange Trading System. Matching engine itself is a batch process system, and the module to get market data and send information back to clients is 
      a online system.
      Percentile/median is better than mean when describe performance. It tells us how many user request are slower than a number(median).
      The response time are different between what client see and what the system see, it is because a slow culprit will make all later client requests wait. System
      only calculate response time as normal, but client actually feels the slowness. So we have two ways to measure performace a little bit accurate:
        1. monitor performance on client side, 2. Sending request independently, don't wait previous request to be finished.
      To calculate percentiles in a sliding window, add up data in a time window and get the result, or use forward decay, t-digest, HdrHistogram.
    Cope increasing load:
      Use a more powerful machine or employ more small elastic machine? Is the service stateless or stateful? Which part of the system is the bottleneck? How about
      maintainance effort we need for new resource?

chapter 2.
  Data model (relational or document) and query language
  Relational DB vs Document DB:
    If application has lots of one to many data, then Document DB (NoSQL) is a good choice, no join is needed and data are easy to be retrieved.
    If we don't a have a fixed schema, we better to use NoSQL database, which is the schema-on-read pattern. It's like RTTI in C++, we cannot assume schema in databse
    until we are reading it. In contrust, relational databse is schema-on-write, we have to follow certain rules when writing.
    When we need to change schema often, NoSQL is preferred in terms of the slowness when altering table in some relational DB. They probably require some downtime.
    If we access set of data in locality pattern, we can use NoSQL, because to join different tables, access indices will require lots of disk IO. NoSQL can
    provide good locality attribute. However, if the amount of data we query is relatively small comparing to cache or we will modify the size of encoded data, it will
    be expensive to achieve them in NoSQL.
  Nowaday, relational and document DB are more and more complementing each other, relational DB has no schema JSON and NoSQL DB resolve relational reference. This 
  hybrid of two models is good route for databases in the future. 
  Query Language:
    declarative query language (SQL, relational algebra, CSS, MapReduce lang, Cypher node4j) vs imperative language
    MapReduce is used by distributed data storage, but not the essential tool. SQL does not constrain to only run in a single node.
    MongoDB use MapReduce, but also have tools like aggregation pipeline to deal with distributed use cases.
    Graph-like data module use vertices and edge to store data and their relation. Any two vertices can connect with each other and user can traverse forward and 
    backward. The relationship of edges can be different, so a graph can represent different information.
    Semantic web and RDF data model: RDF is a machine readible web which similar to the information on the internet which human can understand. There are lots of
    work to be done like standard proposal, complex, etc. (page 57)
chapter 3.
  Hash index: build hash index upon different field can boost reading speed, database engine will figure out which index is needed. However, we cannot store all
    index in memory, we group part of indices into segments and write them into disk. In the meantime, data in segment would have duplication, we need to compact
    them.
    The disadvantage of hash index are 1. when index is in disk, the performance is not very good. 2. when query range of data, it will need to scan the whole hash 
    table
  SSTable and LSM-Trees:
    1. the k-v pairs need to be sorted by key in a segment and one key only appear once in a segment file after merging
    2. We only keep a sparse index table (sorted) in memory, each index point to the offset of key within a sorted segment file(SSTable) which is a small block. 
       If the queried key is in between of two sparse indices, we only need to load the small compressed data block in SSTable
    Steps of building SSTable:
      1. write new data into an in-memory sorted balanced tree which we call memtable.
      2. When the size of memtable exceed threshold, we write it into a SSTable and store it on the disk.
      3. To find a record, we check memtable first, then the SSTable in starting from the latest.
      4. Merge can compact SSTable in the background to save space.
      5. To avoid losing memtable when crash, store log for memtable write without order into disk.
    Performance improvement:
      Bloom filters when read
      Size-tiered compaction is for write intensive db, several small SSTable will be compacted into a larger one, this need 2x size of compacted data disk space.
      Leveled compaction will compact L0 SSTable to higher level continuously. There are two parameters 1. max SSTable size, 2. max size of LevelX.
      https://www.youtube.com/watch?v=TyTXOjFMi7k&t=617s&ab_channel=DataStaxDevelopers
      Max size of LevelX is max SSTable size times X power of 10. L0 is a temporary level to store data, compactor will merge data from all SSTables in L0 and merge
      new data with L1 SSTable, write them into a file not exceeding Max SSTable size and store them into L1. If the size of L1 exceed max size of Lv1, promote and
      merge SSTables in Lv1 with Lv2 SSTables until remaining table in Lv1 fit in max size of L1 and so on.
      https://www.youtube.com/watch?v=6yJEwqseMY4&ab_channel=DataStaxDevelopers
      This is good for reading, when writing happenes, it is very IO intensive
    Log-structured Merge-Tree:
      Instead of calling it as a tree, it is more like a filesystem containning SSTable, MemTable, bloom filter and in memory index table.
  B-tree:
    When B tree insert new record and split a node, DB need to seek and write two pages for split and update parent page, which is a vulnerable operation. Because
    when crash happens, index will be corrupted. We use WAL(write-ahead-log) to protect, it's an append only log on disk, updated before the pages of tree get updated.
    we use latch(lightweight locks) to protect concurrency scenario.
    Optimization:
      Instead of overwriting pages and maintaining WAL for crash recovery, we use copy-on-write which doesn't overwrite original pages but create and copy to a new one
      , it's also good for concurrency.
      We can use abbreviating key instead of real one to add more key in to one node which gives us larger branching factor thus fewer levels, it's good for access key
      Leaf node will add reference to it's sibling left and right allowing scanning keys in order without jumping back to paranet pages.
  Comparing B-Tree and LSM-Tree:
    LSM-Tree is faster for write and B-tree is faster for read, since LSM-tree need to check several different data structure and SSTables.
    Advantage of LSM-tree:
      B-tree must write twice once for WAL one for actual pages when inserting.
      Log-structured indexes also rewrite data multiple times due to compaction and merging which we called write amplification.
      However LSM-tree can typically sustain higher write throughput because of low amplification, and sequentially write compact SSTable is faster then overwrite
      several ramdom pages in a tree, especially on a hard disk.
      LSM-tree is compressed better, it takes smaller disk space. B-tree sometimes leave half of page unused after splitting.
    Disadvantage of LSM-tree:
      LSM-tree need make limited disk IO have to wait for an expensive compaction, this cause high latency at higher percentiles.
      When migrate data into a new DB, the initial write take lots of time because of the on going compaction in the background
      If write rate is high and compaction is not configured well, the compaction cannot keep up with the incoming writes, which easily make disk out of space.
      B-tree only have one copy of key in the tree, but LSM-tree has multiple copies of a key in different SSTables. B-tree can offer strong transactional semantics
      because we can just lock on ranges of keys by attach locks to tree.
  Storing values within the index:
    cluster index(storing all rwo data within the index) and heap file (the file which index reference points to) are two opposite way to refer data.
  Multi-column indexes:
    concatenateed index.
  Full-text search and fuzzy indexes:
    helpful when dealing with misspelled word, works with edit distance.
  In-memory database:
    In-memory database is faster not because of not reading from disk, but it don't need to encoding in-memory data structure in a form that can be written to disk.
    It is also good for storedata models that are difficult to implement with disk-based indexes like priority queue and sets.
    In memory database can deal with data which larger than available memory. The so-called anti-caching approach evict the least recently used data from memory to
    disk, similar as how swap works.
  OLAP and OLTP:
    OLAP data warehouse: large amount of data, read intensively, don't write so much.
    Big company always dump data from different OLTP DB into one OLAP DB for analysis purpose.
    OLAP DB has a fact table in both Stars and Snowflakes module as a contral core. It contains all events and each event row has lot of columns for reference
    of different dimensions.
    Comparing to Stars module, Snowflake has sub-core and breaking down dimension table to get highly normallized attribute.
  Column-Oriented Storage:
    We often query only one or two columns, there is no need to read the whole row. We can store each column of a table in one file, which is very good for compression
    as well. The distinct value in one column is normally much smaller than number of rows, so we use bitmap to represent value in each rows. Each distinct value has
    one bitmap, each row has one bit, if a row has this distinct value, the bit is 1 vice versa.
    Then we encode the bit map to several numbers indicate how many consecutive 0 and 1
    This bitmap can even work well for AND and OR operation
    It is also good for single-instruction-multi-data instruction in mordern CPUs.
    It can fit into L1 cache comfortably.
    
    sorted column storage is good for the compression, the encoded data is very small for the sorted column, even encoded data for other columns is the same size
    it's still a very good optimization. Column-oriented storage can use LSM-tree because it doesn't matter whether the in-memory store is column oriented or 
    row oriented, it will compact, merge and sort anyway.
  Aggregation in OLAP:
    We use materialized view(aggregation) to cache multiple aggregation for reading. We can have several two dimensional tables to store data and we can aggregate
    those table again to get aggregation for one dimension.
Chapter 4. Encoding
  Json, XML, CSV
    Benefit and drawback (pg. 114)
    Binary encoding is very important when transport large data. The naive way to encode JSON waste lots of space.
  Thrift and Protocol Buffers:
    Both Thrift and protocol buffer use number to represent key name, and we only need to check number-key map during decoding, save lots of space if key is long.
    In Thrift BinaryProtocol, encoded data is not so much compacted, but in Thrift CompactProtocol and Protocol Buffer, key number and value type are compacted into
    1 byte, all number value have flexible space instead of 64/32 bits. A lead bit is employed to indicate whether there are still incoming bytes for the value, so
    each byte has 7 bits to store part of value, and 1 bit is the indicator.
    Forward/backward compactibility: we can change the key name, but not he key number, old code will ignore unrecognized key number and type/length field will help
    it to skip. All new fields add after the first version must be optional or have a default value.
  Avro - Hadoop encoding method:
    only length and value are concatenated, same varible size number as Thrift compactprotocol and protocol buffer. No field information, no type information. There
    are reader schema and writer schema to store data type and field name. Reader will get writer schema, comparing with reader schema then parse data. writer schema
    is store in 1. beginning of large data file which under the same writer schema; 2. a table referring to a version number, the version number is in small data set;
    3. setup communication process when RPC begin.
    Avro is good for dynamically generate schema but Thrift have to manually assign field number to a field, and manually update field name.
  For Thrift and protocol buffer, they rely on code generation, which is useful when the programming language is statically type language like C, C++ and Java. They
  allow type checking and efficient in-memory structure for known type. This become an unnecessarily obstacle in dynamically typed languages like Javascript, Python.
  Avro is good for them and Avro also provide code generation for C, C++ and Java.
  
  Model of Dataflow
    Via Database: Using database as a media requires forward and backward compatibility. This need to be considered both in encoding/decoding aspect and application
    aspect.
    Via Network (RPC and REST): Microservice architecture makes each service request and respond to other services. They only expose its api to the outside and good
      for restricting on what a client can and cannot do. Each service only maintained by one team, and the team can upgrade service without having to coordinate with
      other teams. old and new version of server/client should be enable to exist at the same time without any encoding/decoding issue.
      SOAP is an XML based protocol, a client can access a remote service using local classes and method call. it is useful in statically typed language. But different
      extensions in different vendors may cause compatibilty problem.
      RestFul is popular in the context of cross-organizational service integration and microservices.
      RPC flaw: RPC request is unpredictable, it may return nothing without exception, it also may doing something on the server side but no respond, in this case,
      another try will cause duplicated operations. The responsd time largely varies. RPC need to copy all values instead of pass by reference/pointer in local.
      RPC call between two endpoint which use different language becauses ugly. Translating from datatype in A language to type in B language is very unreliable.
      Because of the limitation, RPC often is used on requests between services owned by the same org within the same datacenter. It combining with binary encoding
      usually has better performance than JSON over REST.
      But RESTful api is good for experimentation and debugging, it doesn't need any compilation and web browser/curl can easily be the tool to test the protocol.
      For RPC schema evolution, it's reasonable to assume servers will be update first then client, so server need to be backward compatible and client need to be
        forward compatible. How to manage the compatibility properties of RPC depends on what data encoding method it use. Thrift/gRPC(Protocol Buffer)/Avro RPC use
        their own rules to manage compatibility. RESTful JSON API add optional request parameters and add new fields to response objects to maintain compatibility.
        SOAP use XML schema
    Via Asynchronous message passing systems:
      Client deliver message to another process via the middleware instead of direct network connection. Sender only send message, doesn't expect reply. one process
      can be a sender and a reveiver, but those two parts work in different channels. This is why it is call asynchronous. (RabbitMQ, Kafka, ActiveMQ...)
    Distribute actor framework: a programming model for convurrency in a single process(pg.138)
    
